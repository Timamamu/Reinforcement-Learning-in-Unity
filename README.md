# Reinforcement Learning in Unity – Hummingbird Environment

## Overview
This Unity project demonstrates reinforcement learning through a custom "Hummingbird" training environment built with ML-Agents, where an autonomous bird learns to harvest nectar from flowers distributed throughout a garden arena. The `HummingbirdAgent` uses continuous actions to steer, pitch, and move through 3D space, rewarding successful nectar collection and penalizing collisions with the area boundary during training. The supporting `Flower` and `FlowerArea` components manage nectar availability, flower placement, and reset logic for each training episode.

## Demo


https://github.com/user-attachments/assets/639cbf38-af7e-458e-be71-a4a4f0f867cd




*Watch the trained hummingbird agent autonomously navigate the environment and collect nectar from flowers.*

## Project Structure
```
Assets/
  Hummingbird/          Core environment assets (scripts, prefabs, meshes, textures, scene)
  InputSystem_Actions/  Unity Input System action map used for manual control
  ML-Agents/            Training timer captures generated by past ML-Agents runs
  Settings/             Universal Render Pipeline (URP) assets and profiles
config/
  trainer_config.yaml   ML-Agents training configuration (hyperparameters, network architecture)
Packages/
  manifest.json         Unity package dependencies (ML-Agents, Input System, URP, etc.)
ProjectSettings/        Unity project configuration, including editor version metadata
```

## Requirements
- Unity **6000.2.5f1** (2023 LTS) or newer, matching the editor version used to author the project.
- Unity ML-Agents package **2.0.1** and the new Input System, already declared in `Packages/manifest.json`.
- (Optional) Python 3.8–3.11 with the `mlagents` Python package for local training runs.

## Getting Started
1. Open the repository folder (`Reinforcement-Learning-in-Unity`) in the Unity Hub and launch the project with the matching editor version.
2. Load the **`Assets/Hummingbird/Scenes/Training.unity`** scene to view or play the environment.
3. Select the `HummingbirdAgent` in the hierarchy to tweak hyperparameters such as movement force, pitch/yaw speeds, or training mode before entering Play Mode.

### Using the Pre-Trained Model
A trained neural network model is included in `Assets/Hummingbird/NNModels/` so you can test the learned behavior immediately:

1. Select the `HummingbirdAgent` GameObject in the hierarchy.
2. In the `Behavior Parameters` component, set the **Behavior Type** to *Default*.
3. Assign the `.nn` model from `NNModels/` to the **Model** field.
4. Enter Play Mode to watch the trained agent harvest nectar autonomously.

### Manual Gameplay
Switch the agent's `Behavior Parameters` component to *Heuristic Only* to fly the hummingbird manually. The default key bindings are:

| Action | Keys |
| --- | --- |
| Move forward / backward | `W` / `S` |
| Strafe left / right | `A` / `D` |
| Ascend / descend | `E` / `C` |
| Pitch up / down | `↑` / `↓` |
| Yaw left / right | `←` / `→` |

These bindings map to the heuristics implemented in `HummingbirdAgent.Heuristic`, which feeds keyboard state into the continuous action buffer. An optional `InputSystem_Actions` asset is provided if you prefer to drive the agent via Unity's Input System authoring tools.

### Training with ML-Agents
If you want to train your own model from scratch:

1. Install the Python tooling (once per machine):
```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   pip install mlagents==0.30.0 mlagents-envs==0.30.0
```
2. In Unity, set the agent's `Behavior Parameters` to *Default* and enable **Training Mode** on the `HummingbirdAgent` component to allow automatic resets and boundary penalties.
3. The training configuration is already set up in `config/trainer_config.yaml`, which defines the PPO hyperparameters, network architecture, and reward signals.
4. From the project root, start the training run:
```bash
   mlagents-learn config/trainer_config.yaml --run-id hummingbird_run
```
5. When training converges, the exported `.nn` model will be saved to the `results/` directory. Copy it to `Assets/Hummingbird/NNModels/` and assign it to the agent's `Behavior Parameters` to evaluate the learned policy.

## Environment Details
- **Observations**: The agent logs 10 floats each step, including its rotation, a normalized direction to the nearest flower, alignment dot products, and normalized distance to nectar.
- **Actions**: Five continuous values control translation along local X/Y/Z axes plus pitch and yaw rotation, applied as forces and smooth rotation deltas.
- **Rewards**: Collecting nectar grants positive reward (with alignment bonus), while hitting boundaries during training incurs a penalty. Episodes reset when nectar runs dry or the step limit is reached.
- **Flower Lifecycle**: Each flower refills to full at episode start, changes material color as nectar is depleted, and disables colliders when empty to prevent further collection until reset.

## Troubleshooting
- Ensure no colliders block the agent's spawn position; `MoveToSafeRandomPosition` attempts up to 100 placements and will assert if all fail.
- If nectar colliders cannot be resolved at runtime, verify that prefabs preserve their collider references so `FlowerArea` can populate its lookup dictionary without duplicates.
- Remove or archive existing timer captures under `Assets/ML-Agents/Timers/` if you need a clean slate for profiling future runs.
- If training fails to start, verify that the behavior name in `trainer_config.yaml` matches the **Behavior Name** field in the agent's `Behavior Parameters` component.

## Additional Resources
- [Unity ML-Agents Toolkit Documentation](https://github.com/Unity-Technologies/ml-agents)
